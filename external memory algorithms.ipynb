{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a1af46",
   "metadata": {},
   "source": [
    "**external memory model**\n",
    "\n",
    "**Зачем?**\n",
    "\n",
    "Оперативная память дорогая. К тому же при отключении питания на ней информация не сохраняется, в отличие от жестких дисков. Но на жестких дисках в среднем 7200 оборотов в минуту - это порядка 100 чтений в секунду, то есть **10ms** (RAM:  **~ 100 ns**, то есть в 100 000 раз быстрее) на запрос. В то время как скорость передачи данных порядка 100 МБайт/сек, то есть **$10^9$** бит в секунду. *Так что хочется минимизировать поиск и максимизировать объем загрузки после каждого чтения*\n",
    "\n",
    "Чтобы скомпенсировать время поиска и чтения, мы будем считывать сразу $B$ информации (она порядка МБайта)\n",
    "\n",
    "**B-Search**\n",
    "\n",
    "$B$- поиск вместо бинарного через статичное $B-Tree$ за время $\\mathcal{O}(\\log_{B}n)$. Можно показать, что **быстрее нельзя:** у нас после каждого прочтения $B$ бит возможных $B+1$ вердикта (искомый $x$ левее всех, 1й слева, 2й и т.д.). Рисуем дерево решений и получаем ответ.\n",
    "\n",
    "**B-Merge**\n",
    "\n",
    "При обычном merge мы не хотели делать сразу несколько потоков, потому что там тяжело вычислять минимум. В алгоритмах внешней памяти мы себе это можем позволить и реализовать слияние нескольких потоков с помощью приоритетной очереди.\n",
    "\n",
    "Будем делить массив на $\\frac{M}{B}$ потоков с буфером размера $B$, складывая элементы в приоритетную очередь.\n",
    "Разбиваем на кусочки до тех пор, пока массив не поместится целиком в память. То есть в листьях дерева ветвлений порядка $M$ элементов, а разветвление происходит на $\\frac{M}{B}$ детей. На каждом уровне слияние происходит за порядка $\\frac{n}{B}$ чтений. Как внутри памяти это реализовано (тупым проходом каждый раз, либо же через кучу - это мы **не учитываем**). Суммарно $\\mathcal{O}\\left(\\frac{n}{B}\\log_{\\frac{M}{B}}\\frac{n}{M}\\right)$, на практике глубина порядка 1-2, почти всё считается в памяти после первого же деления.\n",
    "\n",
    "**cache oblivious model**\n",
    "\n",
    "Суть в том, что мы теперь рассматриваем кэш как внешнюю память, но при этом нам неизвестны ничьи размеры. \n",
    "Это интересно из-за того, что мы не можем управлять на практике работой кэша, мы лишь делаем запросы.\n",
    "\n",
    "Обычный merge sort работает за $\\mathcal{O}(\\frac{n}{B}\\log \\frac{n}{M})$, даже несмотря на то, что мы не знаем ни $M$, ни $B$.\n",
    "\n",
    "Но есть нумерация **Ван Эмде Боаса**. \n",
    "* Она строится рекурсивно (и корректное построение я видел только для размеров $2^{2^k}$, с постоянным делением высоты пополам и переходом к $n\\rightarrow \\sqrt{n}$. \n",
    "* В таком случае, каким бы ни было $B$, в какой-то момент рассмотренная нами рекурсивная процедура нумерации станет помещаться целиком внутрь $B$ и мы будем за одно считывание проходить порядка $\\log B$ высоты, при этом всё дерево порядка $\\log n$. \n",
    "* Итого **cache miss'ов $\\log_B n$**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
